{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa198ec",
   "metadata": {},
   "source": [
    "# Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6212044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c596635e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chiangwe/anaconda3/envs/NetHawkes/bin/python\n"
     ]
    }
   ],
   "source": [
    "#============ Importing Packages ============# \n",
    "\n",
    "#--------- Drawing Packages ---------#\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator, NullFormatter, LogLocator)\n",
    "from set_size import set_size\n",
    "from collections import Counter\n",
    "\n",
    "#--------- Tensorflow Packages ---------#\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "#============== Packages for word2vec ==============#\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#============== Packages for classification ==============#\n",
    "from sklearn.linear_model import LinearRegression, PoissonRegressor\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#--------- Utilities Packages ---------#\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import shelve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "import enchant\n",
    "\n",
    "import nltk\n",
    "import obspy\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('word_tokenize')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import words as dict_w\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Scipy Signal\n",
    "from scipy import signal\n",
    "\n",
    "# Detrend the Signal\n",
    "from obspy.signal.detrend import polynomial\n",
    "\n",
    "#--------- Remove Warnings ---------#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfcdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb34989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Read in =========#\n",
    "df = pd.read_csv('Eluvio_DS_Challenge_processes.csv')\n",
    "#display( df.sort_values('up_votes', ascending=False).head(5)['title'].values )\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',  np.unique(df['label']), df['label'])\n",
    "class_weights = dict(zip( np.unique(df['label']), class_weights))\n",
    "\n",
    "\n",
    "df = df[ df['title_clean'].apply(lambda x: type(x)==str) ] \n",
    "y_true = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabc09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9394/9394 - 18s - loss: 0.6642 - binary_accuracy: 0.6194 - val_loss: 0.7127 - val_binary_accuracy: 0.5395\n",
      "Epoch 2/70\n",
      "9394/9394 - 17s - loss: 0.6571 - binary_accuracy: 0.6274 - val_loss: 0.6900 - val_binary_accuracy: 0.5851\n",
      "Epoch 3/70\n",
      "9394/9394 - 17s - loss: 0.6543 - binary_accuracy: 0.6257 - val_loss: 0.6566 - val_binary_accuracy: 0.6177\n",
      "Epoch 4/70\n",
      "9394/9394 - 19s - loss: 0.6526 - binary_accuracy: 0.6267 - val_loss: 0.6384 - val_binary_accuracy: 0.6540\n",
      "Epoch 1/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9394/9394 - 18s - loss: 0.6635 - binary_accuracy: 0.6254 - val_loss: 0.6836 - val_binary_accuracy: 0.5869\n",
      "Epoch 2/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9394/9394 - 18s - loss: 0.6566 - binary_accuracy: 0.6339 - val_loss: 0.7043 - val_binary_accuracy: 0.5687\n",
      "Epoch 3/70\n",
      "9394/9394 - 18s - loss: 0.6543 - binary_accuracy: 0.6376 - val_loss: 0.6825 - val_binary_accuracy: 0.5952\n",
      "Epoch 4/70\n",
      "9394/9394 - 17s - loss: 0.6524 - binary_accuracy: 0.6338 - val_loss: 0.6359 - val_binary_accuracy: 0.6688\n",
      "Epoch 5/70\n",
      "9394/9394 - 18s - loss: 0.6512 - binary_accuracy: 0.6417 - val_loss: 0.6576 - val_binary_accuracy: 0.6446\n",
      "Epoch 1/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9394/9394 - 18s - loss: 0.6657 - binary_accuracy: 0.6209 - val_loss: 0.6664 - val_binary_accuracy: 0.6132\n",
      "Epoch 2/70\n",
      "9394/9394 - 19s - loss: 0.6591 - binary_accuracy: 0.6227 - val_loss: 0.6374 - val_binary_accuracy: 0.6586\n",
      "Epoch 3/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9394/9394 - 19s - loss: 0.6568 - binary_accuracy: 0.6228 - val_loss: 0.6880 - val_binary_accuracy: 0.5727\n",
      "Epoch 4/70\n",
      "9394/9394 - 18s - loss: 0.6550 - binary_accuracy: 0.6252 - val_loss: 0.6686 - val_binary_accuracy: 0.5981\n",
      "Epoch 5/70\n",
      "9394/9394 - 17s - loss: 0.6535 - binary_accuracy: 0.6206 - val_loss: 0.6800 - val_binary_accuracy: 0.5581\n",
      "Epoch 6/70\n",
      "9394/9394 - 17s - loss: 0.6526 - binary_accuracy: 0.6202 - val_loss: 0.6460 - val_binary_accuracy: 0.6299\n"
     ]
    }
   ],
   "source": [
    "def create_model(): \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(30, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(5, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "X_train_in = embed(df['title'].values).numpy()\n",
    "\n",
    "pairs_true_pred = []\n",
    "for each_seed in [42, 50, 123]:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_train_in, y_true, test_size=0.33, random_state=each_seed)\n",
    "    \n",
    "    # Create a basic model instance\n",
    "    model = create_model()\n",
    "\n",
    "    # Display the model's architecture\n",
    "    #model.summary()\n",
    "    \n",
    "    # Callback define\n",
    "    patience = 3; epochs = 70;\n",
    "    checkpoint_filepath = './check_point/01_sim_tdidfNN_mdl.ckpt';\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \\\n",
    "                    epochs=epochs, batch_size=36, verbose=2, class_weight=class_weights,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n",
    "\n",
    "    # Load model and evaluate on test\n",
    "    model = tf.keras.models.load_model('./check_point/01_sim_tdidfNN_mdl.ckpt')\n",
    "    pred_test = model.predict(X_test) > 0.5;\n",
    "    \n",
    "    pairs_true_pred.append([y_test, pred_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f50067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 166557)\n"
     ]
    }
   ],
   "source": [
    "all_true_pred = np.hstack([ np.hstack([np.expand_dims(each[0], 1), each[1]]) for each in pairs_true_pred]).T\n",
    "print(all_true_pred.shape)\n",
    "np.save('SGD_NN_PreTrain.npy', all_true_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e5b4a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 166557)\n",
      "[0.60274534621625397, 0.60867233773125151, 0.61077751693476734]\n",
      "baccu:  0.607398400294\n",
      "recall:  0.670220659037\n",
      "prec:  0.173767957342\n",
      "f1:  0.275926942666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, precision_recall_curve, precision_recall_curve\n",
    "print(all_true_pred.shape)\n",
    "baccu=[]; recall = []; prec =[]; f1=[]; \n",
    "for each in range(0, 3):\n",
    "    baccu.append( balanced_accuracy_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:]) )\n",
    "    recall.append( recall_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    prec.append( precision_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    f1.append( f1_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    \n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "print(baccu)\n",
    "print(\"baccu: \", np.mean(baccu))\n",
    "print(\"recall: \", np.mean(recall))\n",
    "print(\"prec: \", np.mean(prec))\n",
    "print(\"f1: \", np.mean(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d91e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93dbc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b701a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#== Test\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "each_seed = 42; chi_score= ls_chi_score[0];\n",
    "X_train, X_test, y_train, y_test,tol_list_tr, tol_list_te = \\\n",
    "    train_test_split(X_tfidf, y_true, d_index, test_size=0.33, random_state=each_seed)\n",
    "\n",
    "top_k =np.round(X_train.shape[1]/2).astype(int)\n",
    "skb = SelectKBest(chi2, k=top_k ).fit(X_train, y_train)\n",
    "X_train = skb.transform(X_train)\n",
    "X_test = skb.transform(X_test)\n",
    "#=========== Google Trend =============#\n",
    "chi_score_index = np.argsort(chi_score)[::-1]\n",
    "print(type(X_tfidf_google))\n",
    "X_g_train = X_tfidf_google[ np.array(tol_list_tr).T, :]\n",
    "X_g_test = X_tfidf_google[ np.array(tol_list_te).T, :]\n",
    "    \n",
    "pca = PCA(n_components=30)\n",
    "pcscs = pca.fit(X_train.toarray())\n",
    "X_tfidf_pca = pcscs.transform(X_train.toarray())\n",
    "X_test_pca = pcscs.transform(X_test.toarray())\n",
    "\n",
    "pca = IncrementalPCA(n_components=30)\n",
    "pcscs = pca.fit(X_g_train)\n",
    "X_g_train_pca = pcscs.transform(X_g_train)\n",
    "X_g_test_pca = pcscs.transform(X_g_test)\n",
    "\n",
    "X_train = np.concatenate((X_tfidf_pca, X_g_train, X_g_train_pca), axis=1)\n",
    "X_test = np.concatenate(( X_test_pca, X_g_test, X_g_test_pca), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04714fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_tfidf[1000:1010, 0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ecd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer=tf.keras.initializers.Zeros(), input_shape=(input_shape,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation= 'relu', kernel_initializer=tf.keras.initializers.Zeros()))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation= 'relu', kernel_initializer=tf.keras.initializers.Zeros()))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c99221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic model instance\n",
    "model = create_model(X_train.shape[1])\n",
    "\n",
    "# Display the model's architecture\n",
    "#model.summary()\n",
    "\n",
    "# Callback define\n",
    "patience = 3; epochs = 70;\n",
    "checkpoint_filepath = './check_point/01_sim_tdidfNN_mdl.ckpt';\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \\\n",
    "                epochs=epochs, batch_size=36, verbose=2, class_weight=class_weights,\\\n",
    "                callbacks=[early_stopping, model_checkpoint_callback])\n",
    "\n",
    "# Load model and evaluate on test\n",
    "model = tf.keras.models.load_model('./check_point/01_sim_tdidfNN_mdl.ckpt')\n",
    "pred_test = model.predict(X_test) > 0.5;\n",
    "\n",
    "#pairs_true_pred.append([y_test, pred_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f65450",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482aa20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a simple sequential model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pairs_true_pred = []\n",
    "for each_seed, chi_score in zip([42, 50, 123], ls_chi_score):\n",
    "    X_train, X_test, y_train, y_test,tol_list_tr, tol_list_te = \\\n",
    "        train_test_split(X_tfidf, y_true, d_index, test_size=0.33, random_state=each_seed)\n",
    "    \n",
    "    top_k =np.round(X_train.shape[1]/4).astype(int)\n",
    "    skb = SelectKBest(chi2, k=top_k ).fit(X_train, y_train)\n",
    "    X_train = skb.transform(X_train)\n",
    "    X_test = skb.transform(X_test)\n",
    "    \n",
    "    #=========== Google Trend =============#\n",
    "    chi_score_index = np.argsort(chi_score)[::-1]\n",
    "    X_g_train = X_tfidf_google.toarray()[:, chi_score_index[0:top_k] ][ np.array(tol_list_tr), :]\n",
    "    X_g_test = X_tfidf_google.toarray()[:, chi_score_index[0:top_k] ][ np.array(tol_list_te), :]\n",
    "        \n",
    "    pca = PCA(n_components=10)\n",
    "    pcscs = pca.fit(X_train.toarray())\n",
    "    X_tfidf_pca = pcscs.transform(X_train.toarray())\n",
    "    X_test_pca = pcscs.transform(X_test.toarray())\n",
    "    \n",
    "    pcscs = pca.fit(X_g_train)\n",
    "    X_g_train_pca = pcscs.transform(X_g_train)\n",
    "    X_g_test_pca = pcscs.transform(X_g_test)\n",
    "    \n",
    "    X_train = np.concatenate((X_train.toarray(), X_tfidf_pca, X_g_train, X_g_train_pca), axis=1)\n",
    "    \n",
    "    X_test = np.concatenate((X_test.toarray(), X_test_pca, X_g_test, X_g_test_pca), axis=1)\n",
    "    \n",
    "\n",
    "    # Create a basic model instance\n",
    "    model = create_model(X_train.shape[1])\n",
    "\n",
    "    # Display the model's architecture\n",
    "    #model.summary()\n",
    "    \n",
    "    # Callback define\n",
    "    patience = 3; epochs = 70;\n",
    "    checkpoint_filepath = './check_point/01_sim_tdidfNN_mdl.ckpt';\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \\\n",
    "                    epochs=epochs, batch_size=36, verbose=2, class_weight=class_weights,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n",
    "\n",
    "    # Load model and evaluate on test\n",
    "    model = tf.keras.models.load_model('./check_point/01_sim_tdidfNN_mdl.ckpt')\n",
    "    pred_test = model.predict(X_test) > 0.5;\n",
    "    \n",
    "    pairs_true_pred.append([y_test, pred_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_true_pred = np.hstack([ np.hstack([np.expand_dims(each[0], 1), each[1]]) for each in pairs_true_pred]).T\n",
    "print(all_true_pred.shape)\n",
    "np.save('SGD_NN_PCA_google.npy', all_true_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9739f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, precision_recall_curve, precision_recall_curve\n",
    "print(all_true_pred.shape)\n",
    "baccu=[]; recall = []; prec =[]; f1=[]; \n",
    "for each in range(0, 3):\n",
    "    baccu.append( balanced_accuracy_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:]) )\n",
    "    recall.append( recall_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    prec.append( precision_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    f1.append( f1_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    \n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "print(baccu)\n",
    "print(\"baccu: \", np.mean(baccu))\n",
    "print(\"recall: \", np.mean(recall))\n",
    "print(\"prec: \", np.mean(prec))\n",
    "print(\"f1: \", np.mean(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b5c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e01ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a7dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9cf78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_pred = np.vstack([ np.vstack(each) for each in pairs_true_pred])\n",
    "np.save('SGD_PCA.npy', all_true_pred)\n",
    "\n",
    "# ============== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, precision_recall_curve, precision_recall_curve\n",
    "\n",
    "baccu=[]; recall = []; prec =[]; f1=[]; \n",
    "for each in range(0, 3):\n",
    "    baccu.append( balanced_accuracy_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:]) )\n",
    "    recall.append( recall_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    prec.append( precision_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    f1.append( f1_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    \n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "print(\"baccu: \", np.mean(baccu))\n",
    "print(\"recall: \", np.mean(recall))\n",
    "print(\"prec: \", np.mean(prec))\n",
    "print(\"f1: \", np.mean(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ece029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b8d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fdb746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab80253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6525bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66bda20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde20a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Median positive and negative =========#\n",
    "\n",
    "X_train, X_test, y_train, y_test, weight_train, weight_test = \\\n",
    "    train_test_split(X_new, y_true, sample_weight, test_size=0.33, random_state=42)\n",
    "\n",
    "print( np.array([type(each)!=bool for each in y_true]).sum()  ) \n",
    "#class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                 np.unique(y_train),\n",
    "#                                                 y_train)\n",
    "#class_weights = dict(zip( np.unique(y_train), class_weights))\n",
    "#class_weights[True] = class_weights[True]*1.0\n",
    "##\n",
    "## Let's do sample weights\n",
    "#min_pos = df[ df['label'] == True]['up_votes'].min()\n",
    "#max_neg = df[ df['label'] == False]['up_votes'].max()\n",
    "#\n",
    "#\n",
    "#\n",
    "#print(dict(zip( np.unique(y_train), class_weights)) )\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10115659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(2, activation= 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9796b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback define\n",
    "patience = 3; epochs = 70;\n",
    "checkpoint_filepath = './check_point/01_sim_tdidfNN_mdl.ckpt';\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train.toarray(), y_train, validation_data=(X_test.toarray(), y_test), \\\n",
    "                    epochs=epochs, batch_size=36, verbose=2, class_weight=class_weights,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and evaluate on test\n",
    "model = tf.keras.models.load_model('./check_point/01_sim_tdidfNN_mdl.ckpt')\n",
    "pred_test = model.predict(X_test.toarray()) > 0.5;\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "print( \"accuracy_score: \", accuracy_score(y_test, pred_test) )\n",
    "print( \"balanced_accuracy_score: \", balanced_accuracy_score(y_test, pred_test) )\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_test).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21cef68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7351c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try Google trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc75eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12762ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "# Use Token based text embedding trained on English Google News 7B corpus\n",
    "def create_model(): \n",
    "    \n",
    "    embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "    hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51512910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test seprate\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title_clean'], y_true, test_size=0.33, random_state=42)\n",
    "\n",
    "# Callback define\n",
    "patience = 3; epochs = 70;\n",
    "checkpoint_filepath = './check_point/02_pre_nnlm-en-dim50_mdl.ckpt';\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), \\\n",
    "                    epochs=epochs, batch_size=64, verbose=2, sample_weight=weight_train,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and evaluate on test\n",
    "model = tf.keras.models.load_model('./check_point/02_pre_nnlm-en-dim50_mdl.ckpt')\n",
    "pred_test = model.predict(X_test) > 0.5;\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "print( \"accuracy_score: \", accuracy_score(y_test, pred_test) )\n",
    "print( \"balanced_accuracy_score: \", balanced_accuracy_score(y_test, pred_test) )\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_test).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45daee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb7ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "# Use Token based text embedding trained on English Google News 7B corpus\n",
    "# Use pretrain embedding\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "X_train = embed(df['title_clean'].values).numpy()\n",
    "\n",
    "# Train and Test seprate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_true, test_size=0.33, random_state=42)\n",
    "\n",
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense( 2, activation= 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense( 1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ca523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callback define\n",
    "patience = 3; epochs = 70;\n",
    "checkpoint_filepath = './check_point/03_preEmbed_nnlm-en-dim50_mdl.ckpt';\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), \\\n",
    "                    epochs=epochs, batch_size=64, verbose=2, sample_weight=weight_train,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and evaluate on test\n",
    "model = tf.keras.models.load_model('./check_point/03_preEmbed_nnlm-en-dim50_mdl.ckpt')\n",
    "pred_test = model.predict(X_test) > 0.5;\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "print( \"accuracy_score: \", accuracy_score(y_test, pred_test) )\n",
    "print( \"balanced_accuracy_score: \", balanced_accuracy_score(y_test, pred_test) )\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_test).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9876dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262748a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59c9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373b675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c33be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6c08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e41ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db938a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['title_clean'], y_true, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, y=y_train,\n",
    "                    epochs=150, batch_size=32, verbose=2, class_weight=class_weights,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_tfidf, df['up_votes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706aa80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(X_tfidf).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15147d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_tfidf.sum(0).mean() )\n",
    "display(df['up_votes'].mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regr = MLPRegressor(random_state=1, max_iter=500).fit(X_tfidf, df['up_votes'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df['up_votes'].values > df['up_votes'].values.mean()).sum())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = (df['up_votes'].values>np.quantile( df['up_votes'].values, 0.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35858853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_tfidf, y_true )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f94381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c453ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Customize my own metrics\n",
    "\n",
    "class BalancedAccuracy(Metric):\n",
    "    def __init__(self, name=\"balanced_accuracy\", **kwargs):\n",
    "        super(BalancedAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.balanced_accuracy = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = y_pred.nupmy()\n",
    "        y_true = y_true.nupmy()\n",
    "\n",
    "        value = balanced_accuracy_score(y_true, y_pred, sample_weight)\n",
    "        #values = tf.multiply(values, sample_weight)\n",
    "        self.balanced_accuracy.assign_add((value))\n",
    "\n",
    "    def result(self):\n",
    "        return self.balanced_accuracy\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.balanced_accuracy.assign(0.0)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
