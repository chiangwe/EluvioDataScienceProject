{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a16d7c7",
   "metadata": {},
   "source": [
    "# Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b071b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507bfc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chiangwe/anaconda3/envs/NetHawkes/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#============ Importing Packages ============# \n",
    "\n",
    "#--------- Drawing Packages ---------#\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator, NullFormatter, LogLocator)\n",
    "from set_size import set_size\n",
    "from collections import Counter\n",
    "\n",
    "#--------- Tensorflow Packages ---------#\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.metrics import Metric\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "#============== Packages for word2vec ==============#\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#============== Packages for classification ==============#\n",
    "from sklearn.linear_model import LinearRegression, PoissonRegressor\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import keras\n",
    "#--------- Utilities Packages ---------#\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import shelve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import class_weight\n",
    "import enchant\n",
    "\n",
    "import nltk\n",
    "import obspy\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('word_tokenize')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import words as dict_w\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Scipy Signal\n",
    "from scipy import signal\n",
    "\n",
    "# Detrend the Signal\n",
    "from obspy.signal.detrend import polynomial\n",
    "\n",
    "#--------- Remove Warnings ---------#\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b392e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2baacbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Read in =========#\n",
    "df = pd.read_csv('Eluvio_DS_Challenge_processes.csv')\n",
    "#display( df.sort_values('up_votes', ascending=False).head(5)['title'].values )\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',  np.unique(df['label']), df['label'])\n",
    "class_weights = dict(zip( np.unique(df['label']), class_weights))\n",
    "\n",
    "\n",
    "df = df[ df['title_clean'].apply(lambda x: type(x)==str) ] \n",
    "y_true = df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb7538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= TDIDF =========#\n",
    "\n",
    "#print(  df['title_clean'].apply(lambda x: type(x)!=str ).sum()  )\n",
    "bow_converter = CountVectorizer()\n",
    "x = bow_converter.fit_transform(df['title_clean'])\n",
    "\n",
    "words = bow_converter.get_feature_names()\n",
    "\n",
    "bigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[2,2]) \n",
    "trigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3])\n",
    "\n",
    "tfidf_transform = TfidfTransformer(norm=None)\n",
    "X_tfidf = tfidf_transform.fit_transform(x)\n",
    "\n",
    "X_tfidf = normalize(X_tfidf,axis=1)\n",
    "\n",
    "#========= ===  =========#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e616df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': ( X_tfidf.shape[1], ),\n",
    "          'batch_size': 32,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim[0] ))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            # Or load in here\n",
    "            X[i,] =  X_tfidf[i, :]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec81966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, activation='relu', kernel_initializer='he_normal', input_shape=(input_shape,)))\n",
    "    model.add(Dense(10, activation= 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79655349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "WARNING:tensorflow:From /home/chiangwe/anaconda3/envs/NetHawkes/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n",
      "9394/9394 - 37s - loss: 0.6702 - binary_accuracy: 0.5861 - val_loss: 0.6384 - val_binary_accuracy: 0.6451\n",
      "Epoch 2/70\n",
      "9394/9394 - 34s - loss: 0.6464 - binary_accuracy: 0.6134 - val_loss: 0.6135 - val_binary_accuracy: 0.6678\n",
      "Epoch 3/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n",
      "9394/9394 - 34s - loss: 0.6339 - binary_accuracy: 0.6429 - val_loss: 0.6611 - val_binary_accuracy: 0.6136\n",
      "Epoch 4/70\n",
      "9394/9394 - 34s - loss: 0.6203 - binary_accuracy: 0.6591 - val_loss: 0.6543 - val_binary_accuracy: 0.6052\n",
      "Epoch 5/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n",
      "9394/9394 - 34s - loss: 0.6050 - binary_accuracy: 0.6695 - val_loss: 0.6654 - val_binary_accuracy: 0.6057\n",
      "Epoch 6/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n",
      "9394/9394 - 34s - loss: 0.5891 - binary_accuracy: 0.6869 - val_loss: 0.6905 - val_binary_accuracy: 0.5974\n",
      "Epoch 7/70\n",
      "9394/9394 - 34s - loss: 0.5753 - binary_accuracy: 0.7012 - val_loss: 0.6557 - val_binary_accuracy: 0.6322\n",
      "Epoch 8/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n",
      "9394/9394 - 33s - loss: 0.5621 - binary_accuracy: 0.7104 - val_loss: 0.7207 - val_binary_accuracy: 0.5920\n",
      "Epoch 9/70\n",
      "9394/9394 - 34s - loss: 0.5499 - binary_accuracy: 0.7191 - val_loss: 0.6758 - val_binary_accuracy: 0.6376\n",
      "Epoch 10/70\n",
      "9394/9394 - 33s - loss: 0.5399 - binary_accuracy: 0.7254 - val_loss: 0.7170 - val_binary_accuracy: 0.6207\n",
      "Epoch 11/70\n",
      "9394/9394 - 36s - loss: 0.5312 - binary_accuracy: 0.7327 - val_loss: 0.6903 - val_binary_accuracy: 0.6477\n",
      "Epoch 1/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n",
      "9394/9394 - 35s - loss: 0.6692 - binary_accuracy: 0.5747 - val_loss: 0.6793 - val_binary_accuracy: 0.5670\n",
      "Epoch 2/70\n",
      "9394/9394 - 32s - loss: 0.6465 - binary_accuracy: 0.6203 - val_loss: 0.6571 - val_binary_accuracy: 0.6021\n",
      "Epoch 3/70\n",
      "9394/9394 - 32s - loss: 0.6337 - binary_accuracy: 0.6432 - val_loss: 0.6689 - val_binary_accuracy: 0.6165\n",
      "Epoch 4/70\n",
      "9394/9394 - 32s - loss: 0.6160 - binary_accuracy: 0.6650 - val_loss: 0.6617 - val_binary_accuracy: 0.6041\n",
      "Epoch 1/70\n",
      "INFO:tensorflow:Assets written to: ./check_point/01_sim_tdidfNN_mdl.ckpt/assets\n",
      "9394/9394 - 36s - loss: 0.6725 - binary_accuracy: 0.5878 - val_loss: 0.6650 - val_binary_accuracy: 0.6021\n",
      "Epoch 2/70\n",
      "9394/9394 - 34s - loss: 0.6484 - binary_accuracy: 0.6106 - val_loss: 0.6469 - val_binary_accuracy: 0.6214\n",
      "Epoch 3/70\n",
      "9394/9394 - 34s - loss: 0.6351 - binary_accuracy: 0.6276 - val_loss: 0.6576 - val_binary_accuracy: 0.6065\n",
      "Epoch 4/70\n",
      "9394/9394 - 35s - loss: 0.6191 - binary_accuracy: 0.6482 - val_loss: 0.6339 - val_binary_accuracy: 0.6345\n"
     ]
    }
   ],
   "source": [
    "# Define a simple sequential model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pairs_true_pred = []\n",
    "for each_seed in [42, 50, 123]:\n",
    "    X_train, X_test, y_train, y_test, list_tr, list_te = \\\n",
    "        train_test_split(X_tfidf, y_true, range(0, X_tfidf.shape[1]), test_size=0.33, random_state=each_seed)\n",
    "    \n",
    "    # Create a basic model instance\n",
    "    model = create_model(X_train.shape[1])\n",
    "\n",
    "    # Display the model's architecture\n",
    "    #model.summary()\n",
    "    \n",
    "    # Callback define\n",
    "    patience = 3; epochs = 70;\n",
    "    checkpoint_filepath = './check_point/01_sim_tdidfNN_mdl.ckpt';\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "    \n",
    "    # Generators\n",
    "    training_generator = DataGenerator(list_tr, y_train, **params)\n",
    "    validation_generator = DataGenerator(list_te, y_train, **params)\n",
    "\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit_generator(training_generator, y_train, validation_data=validation_generator, \\\n",
    "                    epochs=epochs, batch_size=36, verbose=2, class_weight=class_weights,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n",
    "\n",
    "    # Load model and evaluate on test\n",
    "    model = tf.keras.models.load_model('./check_point/01_sim_tdidfNN_mdl.ckpt')\n",
    "    pred_test = model.predict(X_test.toarray()) > 0.5;\n",
    "    \n",
    "    pairs_true_pred.append([y_test, pred_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18949b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 166557)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_true_pred = np.hstack([ np.hstack([np.expand_dims(each[0], 1), each[1]]) for each in pairs_true_pred]).T\n",
    "print(all_true_pred.shape)\n",
    "np.save('SGD_NN_PCA_orig.npy', all_true_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e04c9256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 166557)\n",
      "[0.57176108886647581, 0.6019019051266763, 0.60190144160449544]\n",
      "baccu:  0.591854811866\n",
      "recall:  0.598351091917\n",
      "prec:  0.170759413345\n",
      "f1:  0.265585035887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, precision_recall_curve, precision_recall_curve\n",
    "print(all_true_pred.shape)\n",
    "baccu=[]; recall = []; prec =[]; f1=[]; \n",
    "for each in range(0, 3):\n",
    "    baccu.append( balanced_accuracy_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:]) )\n",
    "    recall.append( recall_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    prec.append( precision_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    f1.append( f1_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    \n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "print(baccu)\n",
    "print(\"baccu: \", np.mean(baccu))\n",
    "print(\"recall: \", np.mean(recall))\n",
    "print(\"prec: \", np.mean(prec))\n",
    "print(\"f1: \", np.mean(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b4761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf01c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7f806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_pred = np.vstack([ np.vstack(each) for each in pairs_true_pred])\n",
    "np.save('SGD_PCA.npy', all_true_pred)\n",
    "\n",
    "# ============== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72439919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, precision_recall_curve, precision_recall_curve\n",
    "\n",
    "baccu=[]; recall = []; prec =[]; f1=[]; \n",
    "for each in range(0, 3):\n",
    "    baccu.append( balanced_accuracy_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:]) )\n",
    "    recall.append( recall_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    prec.append( precision_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    f1.append( f1_score(all_true_pred[2*each,:], all_true_pred[2*each+1,:], average='binary') )\n",
    "    \n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "print(\"baccu: \", np.mean(baccu))\n",
    "print(\"recall: \", np.mean(recall))\n",
    "print(\"prec: \", np.mean(prec))\n",
    "print(\"f1: \", np.mean(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e8baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813a7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f000293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66a81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71f5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1407d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aacd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========= Median positive and negative =========#\n",
    "\n",
    "X_train, X_test, y_train, y_test, weight_train, weight_test = \\\n",
    "    train_test_split(X_new, y_true, sample_weight, test_size=0.33, random_state=42)\n",
    "\n",
    "print( np.array([type(each)!=bool for each in y_true]).sum()  ) \n",
    "#class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                 np.unique(y_train),\n",
    "#                                                 y_train)\n",
    "#class_weights = dict(zip( np.unique(y_train), class_weights))\n",
    "#class_weights[True] = class_weights[True]*1.0\n",
    "##\n",
    "## Let's do sample weights\n",
    "#min_pos = df[ df['label'] == True]['up_votes'].min()\n",
    "#max_neg = df[ df['label'] == False]['up_votes'].max()\n",
    "#\n",
    "#\n",
    "#\n",
    "#print(dict(zip( np.unique(y_train), class_weights)) )\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b144f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(2, activation= 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7361055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback define\n",
    "patience = 3; epochs = 70;\n",
    "checkpoint_filepath = './check_point/01_sim_tdidfNN_mdl.ckpt';\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train.toarray(), y_train, validation_data=(X_test.toarray(), y_test), \\\n",
    "                    epochs=epochs, batch_size=36, verbose=2, class_weight=class_weights,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and evaluate on test\n",
    "model = tf.keras.models.load_model('./check_point/01_sim_tdidfNN_mdl.ckpt')\n",
    "pred_test = model.predict(X_test.toarray()) > 0.5;\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "print( \"accuracy_score: \", accuracy_score(y_test, pred_test) )\n",
    "print( \"balanced_accuracy_score: \", balanced_accuracy_score(y_test, pred_test) )\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_test).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efb77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try Google trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e2a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "# Use Token based text embedding trained on English Google News 7B corpus\n",
    "def create_model(): \n",
    "    \n",
    "    embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "    hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test seprate\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title_clean'], y_true, test_size=0.33, random_state=42)\n",
    "\n",
    "# Callback define\n",
    "patience = 3; epochs = 70;\n",
    "checkpoint_filepath = './check_point/02_pre_nnlm-en-dim50_mdl.ckpt';\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), \\\n",
    "                    epochs=epochs, batch_size=64, verbose=2, sample_weight=weight_train,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8663a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and evaluate on test\n",
    "model = tf.keras.models.load_model('./check_point/02_pre_nnlm-en-dim50_mdl.ckpt')\n",
    "pred_test = model.predict(X_test) > 0.5;\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "print( \"accuracy_score: \", accuracy_score(y_test, pred_test) )\n",
    "print( \"balanced_accuracy_score: \", balanced_accuracy_score(y_test, pred_test) )\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_test).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6d67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950483b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0cd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "# Use Token based text embedding trained on English Google News 7B corpus\n",
    "# Use pretrain embedding\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "X_train = embed(df['title_clean'].values).numpy()\n",
    "\n",
    "# Train and Test seprate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_true, test_size=0.33, random_state=42)\n",
    "\n",
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense( 2, activation= 'relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense( 1, activation='sigmoid'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callback define\n",
    "patience = 3; epochs = 70;\n",
    "checkpoint_filepath = './check_point/03_preEmbed_nnlm-en-dim50_mdl.ckpt';\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='max')\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False, monitor='val_loss', mode='max', save_best_only=True)\n",
    "\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), \\\n",
    "                    epochs=epochs, batch_size=64, verbose=2, sample_weight=weight_train,\\\n",
    "                    callbacks=[early_stopping, model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1defbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and evaluate on test\n",
    "model = tf.keras.models.load_model('./check_point/03_preEmbed_nnlm-en-dim50_mdl.ckpt')\n",
    "pred_test = model.predict(X_test) > 0.5;\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "print( \"accuracy_score: \", accuracy_score(y_test, pred_test) )\n",
    "print( \"balanced_accuracy_score: \", balanced_accuracy_score(y_test, pred_test) )\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_test).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "print(pred_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967f406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc34ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4e552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d5812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d276e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5296b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb6c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab265f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['title_clean'], y_true, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, y=y_train,\n",
    "                    epochs=150, batch_size=32, verbose=2, class_weight=class_weights,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58adfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_tfidf, df['up_votes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6740b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(X_tfidf).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1bd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_tfidf.sum(0).mean() )\n",
    "display(df['up_votes'].mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f229da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regr = MLPRegressor(random_state=1, max_iter=500).fit(X_tfidf, df['up_votes'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df['up_votes'].values > df['up_votes'].values.mean()).sum())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = (df['up_votes'].values>np.quantile( df['up_votes'].values, 0.50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_tfidf, y_true )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671339ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2889ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Customize my own metrics\n",
    "\n",
    "class BalancedAccuracy(Metric):\n",
    "    def __init__(self, name=\"balanced_accuracy\", **kwargs):\n",
    "        super(BalancedAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.balanced_accuracy = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = y_pred.nupmy()\n",
    "        y_true = y_true.nupmy()\n",
    "\n",
    "        value = balanced_accuracy_score(y_true, y_pred, sample_weight)\n",
    "        #values = tf.multiply(values, sample_weight)\n",
    "        self.balanced_accuracy.assign_add((value))\n",
    "\n",
    "    def result(self):\n",
    "        return self.balanced_accuracy\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.balanced_accuracy.assign(0.0)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
